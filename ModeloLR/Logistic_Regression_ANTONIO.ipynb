{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpas/okCQZi+3njawg1dYl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malcuthrad/FlightDelayHackathonOne/blob/main/Logistic_Regression_ANTONIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicción de Retrasos en Vuelos\n",
        "## Este proyecto implementa un modelo de Regresión Logística para predecir si un vuelo llegará con más de 15 minutos de retraso, utilizando técnicas de balanceo de datos y preprocesamiento avanzado."
      ],
      "metadata": {
        "id": "oJXMkJeoa08h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Instalación y Carga de Librerías\n",
        "## Primero, instalamos las librerías necesarias (especialmente `feature-engine` que no viene por defecto en Colab) e importamos los módulos de procesamiento y visualización."
      ],
      "metadata": {
        "id": "MEVWTErya5KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de feature-engine\n",
        "!pip install feature_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wY-5vLP3GJ8P",
        "outputId": "5f122f70-2cd7-446b-9f4b-90df8ef61366"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feature_engine in /usr/local/lib/python3.12/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->feature_engine) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->feature_engine) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->feature_engine) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.0->feature_engine) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.0->feature_engine) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.11.1->feature_engine) (1.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.11.1->feature_engine) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->feature_engine) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wJcPXqqYDVVa"
      },
      "outputs": [],
      "source": [
        "# Librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from feature_engine.selection import DropConstantFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Carga y Limpieza de Datos\n",
        "## Cargamos el dataset y realizamos una limpieza inicial, eliminando columnas irrelevantes y valores nulos."
      ],
      "metadata": {
        "id": "7VfkwQcubTEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del dataset\n",
        "try:\n",
        "    df = pd.read_csv('vuelos_etl_limpio.csv')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Ensure 'vuelos_etl_limpio.csv' is in the same folder as this script.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8jUtVfsDpUW",
        "outputId": "f8d2f588-2d2e-4f21-9f4f-bd2dbff06841"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversión de fecha y limpieza de columnas no predictivas\n",
        "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
        "df_clean = df.drop(columns=['FL_DATE', 'AIRLINE', 'ORIGIN_CITY', 'DEST_CITY',\n",
        "                            'FL_NUMBER', 'CANCELLED']).dropna()\n",
        "\n",
        "print(f\"Forma del dataset tras limpieza: {df_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqgRDeMeDrb0",
        "outputId": "72e2f6aa-3a89-463e-d8ad-05664fe0606a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma del dataset tras limpieza: (2920056, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Definición del Target y Balanceo\n",
        "## Para evitar que el modelo ignore los retrasos debido al desbalance natural (hay más vuelos a tiempo que retrasados), aplicamos Submuestreo (Downsampling) para tener una proporción 50/50."
      ],
      "metadata": {
        "id": "K7wMt76cbsrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el Target: 1 si el retraso de llegada es > 15 min, 0 de lo contrario\n",
        "df_clean['DELAYED'] = (df_clean['ARR_DELAY'] > 15).astype(int)"
      ],
      "metadata": {
        "id": "6qjUxKPYEesE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BALANCING (Downsampling)\n",
        "\n",
        "# Separación de clases\n",
        "df_delayed = df_clean[df_clean['DELAYED'] == 1]\n",
        "df_ontime = df_clean[df_clean['DELAYED'] == 0]\n",
        "\n",
        "# Reducción de la clase mayoritaria (On-time) para igualar a la minoritaria\n",
        "df_ontime_balanced = df_ontime.sample(n=len(df_delayed), random_state=42)\n",
        "\n",
        "# Combinación y mezcla (shuffle)\n",
        "df_balanced = pd.concat([df_delayed, df_ontime_balanced]).sample(frac=1, random_state=42)\n",
        "\n",
        "print(\"Distribución tras el balanceo:\")\n",
        "print(df_balanced['DELAYED'].value_counts())"
      ],
      "metadata": {
        "id": "GsA386RpEhhl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Ingeniería de Variables (Feature Engineering)\n",
        "## Convertimos los formatos de hora (HH:MM:SS) a minutos totales desde la medianoche para que el modelo pueda procesarlos matemáticamente."
      ],
      "metadata": {
        "id": "OG7dXoOhcjiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def time_to_total_minutes(time_str):\n",
        "    if pd.isna(time_str) or time_str == '': return 0\n",
        "    try:\n",
        "        h, m, s = map(int, str(time_str).split(':'))\n",
        "        return h * 60 + m\n",
        "    except: return 0\n",
        "\n",
        "# Columnas de tiempo a convertir\n",
        "time_cols = ['CRS_DEP_TIME', 'DEP_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'CRS_ARR_TIME', 'ARR_TIME']\n",
        "for col in time_cols:\n",
        "    if col in df_balanced.columns:\n",
        "        df_balanced[col] = df_balanced[col].apply(time_to_total_minutes)"
      ],
      "metadata": {
        "id": "x0D4VVxdEjxd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Preparación de X e y\n",
        "## Eliminamos las columnas de \"fuga de datos\" (data leakage) como `ARR_DELAY` o `DEP_DELAY`, ya que no se conocen antes de que ocurra el vuelo."
      ],
      "metadata": {
        "id": "PC1ddXiQczag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminación de columnas de fuga y variables redundantes\n",
        "cols_to_drop = ['ARR_DELAY', 'DEP_DELAY', 'YEAR', 'MONTH', 'DAY', 'DEP_TIME','ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON']\n",
        "X = df_balanced.drop(columns=cols_to_drop + ['DELAYED'])\n",
        "y = df_balanced['DELAYED']"
      ],
      "metadata": {
        "id": "fcWtKhJBEmrU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificación de variables categóricas (One-Hot Encoding)\n",
        "X = pd.get_dummies(X, columns=['AIRLINE_CODE', 'ORIGIN', 'DEST'], drop_first=True)"
      ],
      "metadata": {
        "id": "49P5g3NYEo0F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. División, Escalado y Entrenamiento\n",
        "## Escalamos los datos para que variables como `DISTANCE` (grandes) no opaquen a las pequeñas, optimizando la Regresión Logística."
      ],
      "metadata": {
        "id": "kE8HHt-NdBdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# División en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "EVmwhfwWEqr2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En este paso se cae colab por falta de RAM\n",
        "\n",
        "# Escalado de variables (Crucial para modelos lineales)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "E6B9x2dGEsiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "NiBqDxVnEutM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Evaluación de Resultados\n",
        "## Generamos la Matriz de Confusión y el Reporte de Clasificación para medir la eficacia."
      ],
      "metadata": {
        "id": "lz-Jn1-JdbfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. EVALUACIÓN\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "OM8wEJpaEybk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados obtenidos corriendo el codigo en Visual Studio"
      ],
      "metadata": {
        "id": "5S4-u2VYZMIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Confusion Matrix\n",
        "\n",
        "* **Class 0:** On Time\n",
        "* **Class 1:** Delayed (Arrival Delay > 15 min)\n",
        "\n",
        "| | Predicted: **On Time** (0) | Predicted: **Delayed** (1) |\n",
        "| :--- | :---: | :---: |\n",
        "| **Actual: On Time** (0) | **73,509** (True Negative) | **29,286** (False Positive) |\n",
        "| **Actual: Delayed** (1) | **37,496** (False Negative) | **65,825** (True Positive) |\n",
        "\n",
        "\n",
        "## 2. Classification Report\n",
        "\n",
        "| Class | Precision | Recall | F1-Score | Support |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **0 (On Time)** | 0.66 | 0.72 | 0.69 | 102,795 |\n",
        "| **1 (Delayed)** | 0.69 | 0.64 | 0.66 | 103,321 |\n",
        "| | | | | |\n",
        "| **Accuracy** | | | **0.68** | **206,116** |\n",
        "| **Macro Avg** | 0.68 | 0.68 | 0.68 | 206,116 |\n",
        "| **Weighted Avg** | 0.68 | 0.68 | 0.68 | 206,116 |"
      ],
      "metadata": {
        "id": "pt0NLOT0YP6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Guardar el Modelo\n",
        "## Guardamos el modelo y el escalador para usarlos en el futuro sin necesidad de volver a entrenar."
      ],
      "metadata": {
        "id": "oOjAfZ_4ePJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardado de archivos .pkl\n",
        "import joblib\n",
        "\n",
        "joblib.dump(log_reg, 'flight_delay_model.pkl')\n",
        "joblib.dump(scaler, 'flight_scaler.pkl')\n",
        "print(\"¡Modelo y Escalador guardados correctamente!\")"
      ],
      "metadata": {
        "id": "O5K28mP7eP3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Código para descargar los archivos .pkl\n",
        "from google.colab import files\n",
        "\n",
        "# Descargar el modelo\n",
        "files.download('flight_delay_model.pkl')\n",
        "\n",
        "# Descargar el escalador\n",
        "files.download('flight_scaler.pkl')\n",
        "\n",
        "# Descargar el features\n",
        "files.download('model_features.pkl')\n"
      ],
      "metadata": {
        "id": "C6i8tMUFfu1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
